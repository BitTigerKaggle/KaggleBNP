file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
# books.idx <- which(colSums(books.mat) > 0)
# books.mat <- books.mat[books.idx, books.idx]
# dim(books.mat)
# replace isbn with book title
## change row name of book.mat, which is a adjacency matrix
## books is the matrix with two columns, respectively isbn and title
change.name <- function(book.mat, books) {
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
return (book.mat)
}
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
# set.seed(1)
# plot(g, layout = layout.fruchterman.reingold, vertex.size = 4, vertex.label.cex = 0.5)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
View(books)
##  task 2
library(igraph)
file.path.books <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/books.csv"
file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
new.row.name <- vector()
for (i in rownames(books.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(books.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(books.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
books.mat <- as.matrix(books.mat)
View(books.mat)
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
# task 1
library(ggplot2)
library(data.table)
library(tm)
library(SnowballC)
library(plyr)
library(lsa)
library(NMF)
# (1)
file.path <- "http://www.yurulin.com/class/fall2015_datamining/data/reuters21578.csv"
reuters <- read.csv(file.path)
# choose the four most popular topics
selected.topics <- sort(table(reuters$topic), decreasing = T)[1:4]
topics <- sort(table(reuters$topic), decreasing = T)
topics <- as.data.frame(topics)
setDT(topics, keep.rownames = TRUE)
colnames(topics) <- c("topic", "count")
qplot(reuters$topic,
geom="histogram",
binwidth = 0.5)
# (2)
# mds.plot <- function() {
selected.topics <- names(selected.topics)
doc.idx = which(reuters$topic %in% selected.topics)
dataset = reuters[doc.idx, ]
# create a corpus
corpus = Corpus(VectorSource(dataset$content))
corpus = tm_map(corpus, content_transformer(tolower)) # to lower case
# corpus = tm_map(corpus, tolower) # to lower case
corpus = tm_map(corpus, removePunctuation) # romove Punctuation
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, function(x) removeWords(x, stopwords("english")))
corpus = tm_map(corpus, stripWhitespace)
# stemDocument(corpus[[1]]) # there is no package called ‘SnowballC’, add it
inspect(corpus[1:3])
corpus <- tm_map(corpus, stemDocument, language="english")
inspect(corpus[1:3])
# corpus <- tm_map(corpus, PlainTextDocument)
td.mat <- TermDocumentMatrix(corpus)
# td.mat = as.matrix(TermDocumentMatrix(corpus))
freq <- findFreqTerms(td.mat, lowfreq=4) # acquire terms in corpus at least 4 times
td.mat <- as.matrix(td.mat)
td.mat <- td.mat[freq,]
dist.mat = dist(t(td.mat)) # compute distance matrix
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = dataset$topic,
id = row.names(dataset))
ggplot(data, aes(x = x, y = y, color = topic)) + geom_point()
# }
# mds.plot()
# (3)
## tfidf
# tfidf.weighting.mds <- function() {
td.mat.w <- lw_tf(td.mat) * gw_idf(td.mat)  ## tf-idf weighting
k = 4
S = svd(as.matrix(td.mat.w), nu = k, nv = k)
u = S$u
s = S$d
v = S$v
td.mat.svd = S$u %*% diag(S$d[1:k]) %*% t(S$v)
dist.mat = dist(t(td.mat.svd))
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = dataset$topic,
id = row.names(dataset))
ggplot(data, aes(x = x, y = y, color = topic)) + geom_point()
# }
# tfidf.weighting.mds()
# las approximated matrix
# lsa.mds <- function() {
lsa.space = lsa(td.mat.w,dims=4)  ## create LSA space
dist.mat = dist(t(as.textmatrix(lsa.space)))  ## compute distance matrix
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = df$topic, id = row.names(df))
ggplot(data, aes(x = x, y = y, color=topic)) + geom_point()
# }
library(ggplot2)
27555 / 2225213
27555 / 591864
1 - 0.01238308
# Hadley's favourite pie chart
df <- data.frame(
variable = c("reviews with tips", "reviews without tips"),
value = c(0.01238308, 0.9876169)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Pac man")
1 - 0.0465563
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("green", "blue")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("green", "blue")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
# reviews with tips 27555
# reviews 2225213
# tips 591864
df <- data.frame(
variable = c("reviews with tips", "reviews without tips"),
value = c(0.01238308, 0.9876169)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
d <- c("average length of tips in review"= 13.1, "average length of tips not in review" = 10.7)
d <- as.matrix(d)
colnames(d) <- "avg_len"
d <- as.data.frame(d)
qplot(factor(avg_len), data=d, geom="bar", fill=factor(avg_len))
library(ggplot2)
d <- c("average length of tips in review"= 13.1, "average length of tips not in review" = 10.7)
d <- as.matrix(d)
colnames(d) <- "avg_len"
d <- as.data.frame(d)
qplot(factor(avg_len), data=d, geom="bar", fill=factor(avg_len))
View(d)
d[1]
d[1,]
dim(d)
g <- ggplot(d, aes(avg_len))
# Number of cars in each class:
g + geom_bar()
counts <- table(g$avg_len)
barplot(counts, main="Car Distribution",
xlab="Number of Gears")
counts <- table(d$avg_len)
barplot(counts, main="Car Distribution",
xlab="Number of Gears")
mtcar
mtcars
d
dim(d)
dim(mtcars)
counts <- table(mtcars$gear)
barplot(counts, main="Car Distribution",
xlab="Number of Gears")
library(MASS)
avg_len = d$avg_len
barplot(avg_len)
mpg
g <- ggplot(mpg, avg_len)
# Number of cars in each class:
g + geom_bar()
g <- ggplot(d, avg_len)
# Number of cars in each class:
g + geom_bar()
?barplot
library(MASS)
avg_len = d$avg_len
barplot(avg_len, legend.text = c("avg_len for tips in review", "avg_len for tips not in review"))
ggplot(data=d, aes(x=len, y=avg_len, fill=len)) +
geom_bar(colour="black", stat="identity")
dat <- data.frame(
name = factor(c("avg_len for tips in review","avg_len for tips not in review"), levels=c("avg_len for tips in review","avg_len for tips not in review")),
len = c(13.1, 10.7)
)
dat
ggplot(data=dat, aes(x=name, y=len, fill=name)) +
geom_bar(colour="black", stat="identity") +
guides(fill=FALSE)
ggplot(data=dat, aes(x=name, y=len, fill=name)) +
geom_bar(colour="black", stat="identity") + xlab("two kinds of tips") + ylab("length of tips") +
guides(fill=FALSE)
setwd("~/GitHub/KaggleBNP")
library(readr)
cat("reading data, including train and test\n")
train <- read_csv("train.csv")
test <- read_csv("test.csv")
train$target <- as.factor(train$target)
target <- as.data.frame(train$target)
cat("remove ID and target variable ")
train <- train[, -c(1:2)]
test <- test[, -1]
cat("check missing value percentile\n")
mean(is.na(train)) ## overall missing value
mean(is.na(test))
missing_train <- apply(train, 2, function(x) sum(is.na(x))/length(x))
missing_test <- apply(test, 2, function(x) sum(is.na(x))/length(x))
cat("remove constant\n")
col.const <- sapply(train, function(x) length(unique(x))==1)
table(col.const) # no constant columns
rm(col.const)
cat("remove duplicated columns\n")
table(duplicated(as.list(train))) # no duplicated columns
cat("separate numeric and non numeric columns\n")
train.num <- train[, sapply(train, is.numeric)]
train.char <- train[, sapply(train, is.character)]
test.num <- test[, sapply(test, is.numeric)]
test.char <- test[, sapply(test, is.character)]
cat("convert categorical to numeric")
for (f in names(train.char)) {
if (class(train.char[[f]])=="character") {
levels <- unique(c(train.char[[f]], test.char[[f]]))
train.char[[f]] <- as.integer(factor(train.char[[f]], levels=levels))
test.char[[f]]  <- as.integer(factor(test.char[[f]],  levels=levels))
}
}
rm(f, levels)
cat("find and remove redundant variables for numeric")
library(corrplot)
library(caret)
corr.Matrix <- cor(train.num, use="pairwise.complete.obs")
corr.75 <- findCorrelation(corr.Matrix, cutoff = 0.75)
train.num.75 <- train.num[, -corr.75]
corrplot(corr.Matrix, order = "hclust")
corr.85 <- findCorrelation(corr.Matrix, cutoff = 0.85)
train.num.85 <- train.num[, -corr.85]
corr.90 <- findCorrelation(corr.Matrix, cutoff = 0.90)
train.num.90 <- train.num[, -corr.90]
cat("imputation on missing value")
train.num.75[is.na(train.num.75)] <- -1
train1 <- cbind(train.num.75, train.char)
train1 <- cbind(train1, target)
train1[is.na(train1)] <- -1
train.num.85[is.na(train.num.85)] <- -1
train2 <- cbind(train.num.85, train.char)
train2 <- cbind(train2, target)
train2[is.na(train2)] <- -1
train.num.90[is.na(train.num.90)] <- -1
train3 <- cbind(train.num.90, train.char)
train3 <- cbind(train3, target)
train3[is.na(train3)] <- -1
rm(train.num.75, train.num.85, train.num.90, train1, train2, train3, corr.75, corr.85, corr.90)
corr.80 <- findCorrelation(corr.Matrix, cutoff = 0.80)
train.num.80 <- train.num[, -corr.80]
corrplot(corr.Matrix, order = "hclust")
corr.90 <- findCorrelation(corr.Matrix, cutoff = 0.90)
train.num.90 <- train.num[, -corr.90]
cat("imputation on missing value")
train.num.80[is.na(train.num.80)] <- -1
train1 <- cbind(train.num.80, train.char)
train1 <- cbind(train1, target)
train1[is.na(train1)] <- -1
train.num.90[is.na(train.num.90)] <- -1
train2 <- cbind(train.num.90, train.char)
train2 <- cbind(train2, target)
train2[is.na(train2)] <- -1
library(FSelector)
weights <- symmetrical.uncertainty(target~., train1)
print(weights)
subset <- cutoff.k(weights, 50)
train.clean <- train1[, subset]
str(train1)
library(FSelector)
weights <- symmetrical.uncertainty(target~., train1)
temp.target <- target
rm(target)
weights <- symmetrical.uncertainty(target~., train1)
str(train1)
weights <- symmetrical.uncertainty(train$target~., train1)
missing_train <- apply(train, 1, function(x) sum(is.na(x)))
head(missing_train)
View(train)
?apply
rm(missing_test, missing_train)
cat("missing value count per observation as a predictor")
count.missing.train <- apply(train, 1, function(x) sum(is.na(x)))
count.missing.test <- apply(test, 1, function(x) sum(is.na(x)))
setwd("~/GitHub/KaggleBNP")
library(readr)
cat("reading data, including train and test\n")
train <- read_csv("train.csv")
test <- read_csv("test.csv")
train$target <- as.factor(train$target)
target <- train$target
target <- as.data.frame(target)
cat("remove ID and target variable ")
train <- train[, -c(1:2)]
test <- test[, -1]
cat("missing value count per observation as a predictor")
count.missing.train <- apply(train, 1, function(x) sum(is.na(x)))
count.missing.test <- apply(test, 1, function(x) sum(is.na(x)))
count.missing.train <- as.data.frame(count.missing.train)
count.missing.test <- as.data.frame(count.missing.test)
head(count.missing.train)
cat("separate numeric and non numeric columns\n")
train.num <- train[, sapply(train, is.numeric)]
train.char <- train[, sapply(train, is.character)]
test.num <- test[, sapply(test, is.numeric)]
test.char <- test[, sapply(test, is.character)]
cat("convert categorical to numeric")
for (f in names(train.char)) {
if (class(train.char[[f]])=="character") {
levels <- unique(c(train.char[[f]], test.char[[f]]))
train.char[[f]] <- as.integer(factor(train.char[[f]], levels=levels))
test.char[[f]]  <- as.integer(factor(test.char[[f]],  levels=levels))
}
}
rm(f, levels)
cat("find and remove redundant variables for numeric")
library(corrplot)
library(caret)
corr.Matrix <- cor(train.num, use="pairwise.complete.obs")
corr.80 <- findCorrelation(corr.Matrix, cutoff = 0.80)
train.num.80 <- train.num[, -corr.80]
corrplot(corr.Matrix, order = "hclust")
corr.90 <- findCorrelation(corr.Matrix, cutoff = 0.90)
train.num.90 <- train.num[, -corr.90]
cat("imputation on missing value")
# train.num.80[is.na(train.num.80)] <- -1
train1 <- cbind(train.num.80, train.char)
train1 <- cbind(train1, target)
train1[is.na(train1)] <- -1
# train.num.90[is.na(train.num.90)] <- -1
train2 <- cbind(train.num.90, train.char)
train2 <- cbind(train2, target)
train2[is.na(train2)] <- -1
cat("feature ranking, including correlation and entropy based")
library(FSelector)
weights <- symmetrical.uncertainty(train$target~., train1)
print(weights)
subset <- cutoff.k(weights, 50)
train.clean1 <- train1[, subset]
cat("feature ranking, including correlation and entropy based")
library(FSelector)
weights <- symmetrical.uncertainty(target~., train1)
print(weights)
subset <- cutoff.k(weights, 50)
train.clean1 <- train1[, subset]
weights2 <- symmetrical.uncertainty(target~., train2)
print(weights2)
subset2 <- cutoff.k(weights2, 60)
train.clean2 <- train1[, subset]
weights2 <- symmetrical.uncertainty(target~., train2)
print(weights2)
subset2 <- cutoff.k(weights2, 60)
train.clean2 <- train2[, subset]
View(count.missing.test)
test[is.na(test)] <- -1
test.clean.80 <- test[, colnames(train.clean)]
test.clean.80 <- test[, colnames(train.clean1)]
train.clean2 <- train2[, subset2]
test.clean.80 <- cbind(test.clean.80, count.missing.test)
test.clean.90 <- test[, colnames(train.clean2)]
test.clean.90 <- cbind(test.clean.90, count.missing.test)
train.clean2 <- cbind(train.clean2, target)
train.clean2 <- cbind(train.clean2, count.missing.train)
cat("write cleaning files to disk")
write.csv(train.clean1, "train_clean1.csv", row.names = F)
saveRDS(train.clean1, "train_clean1.rds")
write.csv(train.clean2, "train_clean2.csv", row.names = F)
saveRDS(train.clean2, "train_clean2.rds")
write.csv(test.clean.80, "test_clean1.csv", row.names = F)
saveRDS(test.clean.80, "test_clean1.rds")
write.csv(test.clean.90, "test_clean2.csv", row.names = F)
saveRDS(test.clean.90, "test_clean2.rds")
