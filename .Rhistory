title("closeness")
}
betweenness.centrality <- function(books.mat) {
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node betweenness centrality
bet = betweenness(g)
top = order(bet, decreasing = T)[1:5]
## size node by betweenness
V(g)$size = abs(bet)*0.1
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("betweenness")
}
page.rank.centrality <- function(books.mat) {
# PageRank centrality
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node PageRank centrality
bet = page_rank(g)
top = order(bet$vector, decreasing = T)[1:5]
## size node by betweenness
V(g)$size = abs(bet$vector)*150
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("PageRank")
}
degree.centrality(books.mat)
closeness.centrality(books.mat)
betweenness.centrality(books.mat)
page.rank.centrality(books.mat)##  task 2
library(igraph)
file.path.books <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/books.csv"
file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
# books.idx <- which(colSums(books.mat) > 0)
# books.mat <- books.mat[books.idx, books.idx]
# dim(books.mat)
# replace isbn with book title
## change row name of book.mat, which is a adjacency matrix
## books is the matrix with two columns, respectively isbn and title
change.name <- function(book.mat, books) {
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- names.title
colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
return (book.mat)
}
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
# set.seed(1)
# plot(g, layout = layout.fruchterman.reingold, vertex.size = 4, vertex.label.cex = 0.5)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
## (3)
# Identify the most central nodes in the network based on different centrality measures, degree centrality,
# closeness centrality, betweenness centrality, and PageRank.
# Plot different networks where the nodes are sized based on the centrality measures.
# Highlight the top 5 nodes with the highest centrality measures in each network.
degree.centrality <- function(books.mat) {
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
# compute node degree centrality
deg <- degree(g)
top <- order(deg, decreasing = T)[1:5]  ## the top-5 nodes with highest degrees
## size node by degree
V(g)$size = abs(deg) * 1
V(g)$color = "gray"
V(g)$label.color = "gray"
V(g)$label.cex = 0.5
E(g)$color = "black"
V(g)[top]$label.color = "black"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
V(g)[top]$color = "Skyblue"
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("degree centrality")
}
closeness.centrality <- function(books.mat) {
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node closeness centrality
clo = closeness(g)
top = order(clo, decreasing = T)[1:5]
## size node by closeness
V(g)$size = abs(clo)^2 * 1e+05*3
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("closeness")
}
betweenness.centrality <- function(books.mat) {
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node betweenness centrality
bet = betweenness(g)
top = order(bet, decreasing = T)[1:5]
## size node by betweenness
V(g)$size = abs(bet)*0.1
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("betweenness")
}
page.rank.centrality <- function(books.mat) {
# PageRank centrality
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## compute node PageRank centrality
bet = page_rank(g)
top = order(bet$vector, decreasing = T)[1:5]
## size node by betweenness
V(g)$size = abs(bet$vector)*150
V(g)$color = "gray"
V(g)$label.color = "black"
V(g)$label.cex = 0.4
V(g)[top]$label.color = "red"  ## highlight the top-5 nodes
V(g)[top]$label.cex = 1
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
title("PageRank")
}
degree.centrality(books.mat)
closeness.centrality(books.mat)
betweenness.centrality(books.mat)
page.rank.centrality(books.mat)
View(books.mat)
##  task 2
library(igraph)
file.path.books <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/books.csv"
file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
# books.idx <- which(colSums(books.mat) > 0)
# books.mat <- books.mat[books.idx, books.idx]
# dim(books.mat)
# replace isbn with book title
## change row name of book.mat, which is a adjacency matrix
## books is the matrix with two columns, respectively isbn and title
change.name <- function(book.mat, books) {
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
return (book.mat)
}
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
# set.seed(1)
# plot(g, layout = layout.fruchterman.reingold, vertex.size = 4, vertex.label.cex = 0.5)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
View(books)
##  task 2
library(igraph)
file.path.books <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/books.csv"
file.path.ratings <- "http://www.yurulin.com/class/fall2015_datamining/data/Book_Crossing/ratings.csv"
books <- read.csv(file.path.books, colClasses = c("character", "character"))
ratings <- read.csv(file.path.ratings, colClasses = c("integer", "character", "integer"))
d1 <- subset(ratings, rating >= 5) # ratings >= 5
ratings.top <- sort(table(d1$isbn), decreasing = T) # sort # of rated movies
ratings.top <- rownames(ratings.top[1:30]) # get top 30
## create book-book co-rated network
df <- subset(d1, isbn %in% ratings.top)
g <- graph.data.frame(df, directed=T)
mat <- get.adjacency(g)
m2 <- t(as.matrix(mat)) %*% as.matrix(mat)
books.idx <- which(colSums(m2) > 0)
books.mat <- m2[books.idx, books.idx]
diag(books.mat) = 0 ## co-rated with self does not count
new.row.name <- vector()
for (i in rownames(book.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(book.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(book.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
book.mat <- as.matrix(book.mat)
new.row.name <- vector()
for (i in rownames(books.mat)) {
for (j in 1:31424) {
if (i == books[j,1]) {
new.row.name <- c(new.row.name, books[j,2])
}
}
}
book.mat <- as.data.frame(book.mat)
row.names(books.mat) <- NULL
# colnames(book.mat) <- NULL
row.names(books.mat) <- new.row.name
colnames(books.mat) <- new.row.name
##row.names(book.mat) <- names.title
##colnames(books.mat) <- names.title
books.mat <- as.matrix(books.mat)
View(books.mat)
books.mat <- change.name(books.mat, books)
# List the names of the top 10 books and their number of ratings
books.order <- rowSums(books.mat)
books.order <- sort(books.order, decreasing = T)
books.order.10 <- books.order[1:10]
g <- graph.adjacency(books.mat, weighted = T, mode = "undirected", diag = F)
# E(g)$weight # to see the weights of all edges
g=delete.edges(g, which(E(g)$weight <= 2)) # delete edges that have their weight < 3
## g=delete.vertices(g,which(degree(g)<1)) # leaves a skinny graph with a few detached vertices.
set.seed(1)
plot(g, layout = layout.fruchterman.reingold, vertex.label = V(g)$name)
## (2) Identify the community structure in the network by using the modularity-based community detection algorithm.
fc = fastgreedy.community(g)
modularity(fc)
# Plot the network with the detected community structure
set.seed(1)
plot(fc, g, main = "modularity community", layout = layout.fruchterman.reingold,
vertex.size = 4, vertex.label.cex = 0.5)
# plot the dendrogram
dendPlot(fc)
# task 1
library(ggplot2)
library(data.table)
library(tm)
library(SnowballC)
library(plyr)
library(lsa)
library(NMF)
# (1)
file.path <- "http://www.yurulin.com/class/fall2015_datamining/data/reuters21578.csv"
reuters <- read.csv(file.path)
# choose the four most popular topics
selected.topics <- sort(table(reuters$topic), decreasing = T)[1:4]
topics <- sort(table(reuters$topic), decreasing = T)
topics <- as.data.frame(topics)
setDT(topics, keep.rownames = TRUE)
colnames(topics) <- c("topic", "count")
qplot(reuters$topic,
geom="histogram",
binwidth = 0.5)
# (2)
# mds.plot <- function() {
selected.topics <- names(selected.topics)
doc.idx = which(reuters$topic %in% selected.topics)
dataset = reuters[doc.idx, ]
# create a corpus
corpus = Corpus(VectorSource(dataset$content))
corpus = tm_map(corpus, content_transformer(tolower)) # to lower case
# corpus = tm_map(corpus, tolower) # to lower case
corpus = tm_map(corpus, removePunctuation) # romove Punctuation
corpus = tm_map(corpus, removeNumbers) # remove numbers
corpus = tm_map(corpus, function(x) removeWords(x, stopwords("english")))
corpus = tm_map(corpus, stripWhitespace)
# stemDocument(corpus[[1]]) # there is no package called ‘SnowballC’, add it
inspect(corpus[1:3])
corpus <- tm_map(corpus, stemDocument, language="english")
inspect(corpus[1:3])
# corpus <- tm_map(corpus, PlainTextDocument)
td.mat <- TermDocumentMatrix(corpus)
# td.mat = as.matrix(TermDocumentMatrix(corpus))
freq <- findFreqTerms(td.mat, lowfreq=4) # acquire terms in corpus at least 4 times
td.mat <- as.matrix(td.mat)
td.mat <- td.mat[freq,]
dist.mat = dist(t(td.mat)) # compute distance matrix
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = dataset$topic,
id = row.names(dataset))
ggplot(data, aes(x = x, y = y, color = topic)) + geom_point()
# }
# mds.plot()
# (3)
## tfidf
# tfidf.weighting.mds <- function() {
td.mat.w <- lw_tf(td.mat) * gw_idf(td.mat)  ## tf-idf weighting
k = 4
S = svd(as.matrix(td.mat.w), nu = k, nv = k)
u = S$u
s = S$d
v = S$v
td.mat.svd = S$u %*% diag(S$d[1:k]) %*% t(S$v)
dist.mat = dist(t(td.mat.svd))
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = dataset$topic,
id = row.names(dataset))
ggplot(data, aes(x = x, y = y, color = topic)) + geom_point()
# }
# tfidf.weighting.mds()
# las approximated matrix
# lsa.mds <- function() {
lsa.space = lsa(td.mat.w,dims=4)  ## create LSA space
dist.mat = dist(t(as.textmatrix(lsa.space)))  ## compute distance matrix
doc.mds = cmdscale(dist.mat, k = 2)
data = data.frame(x = doc.mds[, 1], y = doc.mds[, 2], topic = df$topic, id = row.names(df))
ggplot(data, aes(x = x, y = y, color=topic)) + geom_point()
# }
library(ggplot2)
27555 / 2225213
27555 / 591864
1 - 0.01238308
# Hadley's favourite pie chart
df <- data.frame(
variable = c("reviews with tips", "reviews without tips"),
value = c(0.01238308, 0.9876169)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Pac man")
1 - 0.0465563
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("green", "blue")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("green", "blue")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
# reviews with tips 27555
# reviews 2225213
# tips 591864
df <- data.frame(
variable = c("reviews with tips", "reviews without tips"),
value = c(0.01238308, 0.9876169)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
df <- data.frame(
variable = c("tips in review", "tips not in review"),
value = c(0.0465563, 0.9534437)
)
ggplot(df, aes(x = "", y = value, fill = variable)) +
geom_bar(width = 1, stat = "identity") +
scale_fill_manual(values = c("blue", "green")) +
coord_polar("y", start = pi / 3) +
labs(title = "Yelp")
setwd("~/GitHub/KaggleBNP")
library(readr)
cat("reading data, including train and test\n")
# path <- "/Users/fujun/Desktop/DataMining/"
train <- read_csv("train.csv", collapse = "")
rm(all)
rm()
train <- read_csv("train.csv")
View(train)
dim(train)
head(train)
str(train)
mean(is.na(train))
apply(train, 2, function(x) sum(is.na(x))/length(x))
test <- read_csv("test.csv")
cat("check missing value percentile\n")
mean(is.na(train)) ## overall missing value
mean(is.na(test))
col.const <- sapply(train, function(x) length(unique(x))==1)
col.const
missing_train <- apply(train, 2, function(x) sum(is.na(x))/length(x))
missing_test <- apply(test, 2, function(x) sum(is.na(x))/length(x))
missing_test
View(test)
missing_train
str(train)
cat("remove duplicated columns\n")
table(duplicated(as.list(train)))
head(train)
str(train)
cat("separate numeric and non numeric columns\n")
train.num <- train[, sapply(train, is.numeric)]
train.char <- train[, sapply(train, is.character)]
View(train.char)
test.num <- test[, sapply(train, is.numeric)]
test.char <- test[, sapply(test, is.character)]
cat("separate numeric and non numeric columns\n")
train.num <- train[, sapply(train, is.numeric)]
train.char <- train[, sapply(train, is.character)]
test.num <- test[, sapply(test, is.numeric)]
test.char <- test[, sapply(test, is.character)]
col.const
rm(col.const)
char.names <- names(train.char)
for (f in char.names) {
levels <- unique(c(train.char[[f]], test.char[[f]]))
train.char[[f]] <- factor(train.char[[f]], levels=levels)
test.char[[f]]  <- factor(test.char[[f]],  levels=levels)
}
rm(char.names, f, levels)
str(train.char)
str(test.char)
summary(train.char)
dim(trai)
dim(train)
str(train.char)
